{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Machine Learning\n",
    "\n",
    "### *Session \\#5*\n",
    "\n",
    "\n",
    "### Helpful shortcuts\n",
    "---\n",
    "\n",
    "**SHIFT** + **ENTER** ----> Execute Cell\n",
    "\n",
    "**UP/DOWN ARROWS** --> Move cursor between cells (then ENTER to start typing)\n",
    "\n",
    "**TAB** ----> See autocomplete options\n",
    "\n",
    "**ESC** then **b** ----> Create Cell \n",
    "\n",
    "**ESC** then **dd** ----> Delete Cell\n",
    "\n",
    "**\\[python expression\\]?** ---> Explanation of that Python expression\n",
    "\n",
    "**ESC** then **m** then __ENTER__ ----> Switch to Markdown mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Probability and Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "---\n",
    "\n",
    "**You are planning a picnic, but it's cloudy outside! Should we still go?**\n",
    "\n",
    "**1. We have data on 300 days, and 135 of them are cloudy. What's the probability of a cloudy day,** `P(C)`**?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "135/300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. It only rains on 30 out of 300 days. What's the probability of rain,** `P(R)`**?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30/300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Of the rainy days, 15 of them were cloudy. What's the probability of** `P(C|R)` **?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15/30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Use Bayes' Theorem to find the chance of rain, given that it's cloudy:** `P(R|C)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11111111111111112"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P(C|R)P(R)/P(C)\n",
    "\n",
    "15./30.*30./300./(135./300.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. New example: Are you being cheated in a coin flip? Assume that 9 out of 10 coins are fair.**\n",
    "\n",
    "**What's the probability you're dealing with a fair coin** `P(F)`**? Probability of an unfair coin** `P(U)`**?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# P(F)\n",
    "9/10 \n",
    "\n",
    "# P(U)\n",
    "1/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Now you see the coin land heads 4 times in a row.** \n",
    "\n",
    "**What's the probability of this happening with a fair coin, i.e.** `P(HHHH|F)` **?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0625\n"
     ]
    }
   ],
   "source": [
    "# P(HHHH|F)\n",
    "\n",
    "hhhh_f = 0.5 * 0.5 * 0.5 * 0.5\n",
    "print(hhhh_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Unfair coins land heads 90% of the time.** \n",
    "\n",
    "**What's the probability of this happening with an unfair coin, i.e.** `P(HHHH|U)` **?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6561000000000001\n"
     ]
    }
   ],
   "source": [
    "# P(HHHH|U)\n",
    "\n",
    "hhhh_u = 0.9 * 0.9 * 0.9 * 0.9\n",
    "print(hhhh_u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Use Bayes Theorem to compute the probability that the coin is unfair, i.e.** `P(U|HHHH)` \n",
    "\n",
    "Hint: You'll need `P(HHHH)`, the overall chance of getting 4 heads. To get this we need to combine `P(HHHH|F)` and `P(HHHH|U)`, weighted for how likely each type of coin is.\n",
    "\n",
    "Summed up:  `P(HHHH)` = `P(HHHH|F)P(F)` + `P(HHHH|U)P(U)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5384047267355982\n"
     ]
    }
   ],
   "source": [
    "f=0.9\n",
    "u=0.1\n",
    "hhhh = hhhh_f * f + hhhh_u * u\n",
    "u_hhhh = hhhh_u * u / hhhh\n",
    "print(u_hhhh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm Ups\n",
    "\n",
    "*Type the given code into the cell below*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from yellowbrick.model_selection import ValidationCurve\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"spam.csv\", encoding = 'latin-1')\n",
    "X = df['text']\n",
    "y = df['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize the words:** \n",
    "```\n",
    "vec = CountVectorizer(stop_words='english', max_features=20)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mvec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m())\n\u001b[1;32m      6\u001b[0m vec\u001b[38;5;241m.\u001b[39mtransform(X)\u001b[38;5;241m.\u001b[39mtoarray() \u001b[38;5;66;03m#Translate the emails into vectors\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(stop_words='english', max_features=20)\n",
    "vec.fit(X) #Creates a separate column for each word\n",
    "print(X.iloc[2])\n",
    "print('')\n",
    "print(vec.get_feature_names())\n",
    "vec.transform(X).toarray() #Translate the emails into vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create pipeline:** \n",
    "```\n",
    "pipe = make_pipeline(vec, MultinomialNB())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(vec, MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation Curve:** \n",
    "```python\n",
    "model_vc = ValidationCurve(pipe, 'multinomialnb__alpha', range(1, 12))\n",
    "model_vc.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vc = ValidationCurve(pipe, 'multinomialnb__alpha', range(1, 12))\n",
    "model_vc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "---\n",
    "\n",
    "**1. Create a variable** `ham` **for the text from rows of** `df` **that are categorized as ham.** \n",
    "\n",
    "**Likewise, create a variable** `spam` **for the text from rows of** `df` **that are categorized as spam**\n",
    "\n",
    "\n",
    "Hint: Use boolean fitering first, then grab the `text` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = df[df['category'] == 'ham'].text\n",
    "spam = df[df['category'] == 'spam'].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The code below will fetch the number of times each feature (ie word) occurs**\n",
    "\n",
    "**Use** `plt.barh()` **to show a histogram of the most common** `ham` **words**\n",
    "\n",
    "Hint: `.barh()` expects a list of labels, then a list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_counts = np.asarray(vec.fit_transform(ham).sum(axis=0))[0]\n",
    "ham_features = vec.get_feature_names()\n",
    "\n",
    "plt.barh(ham_features, ham_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Now fit** `vec` **to the** `spam` **dataset. Adapt the code above to create** `spam_counts` **and** `spam_features`\n",
    "\n",
    "**Use** `plt.barh()` **to show a histogram of the most common** `spam` **words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_counts = np.asarray(vec.fit_transform(spam).sum(axis=0))[0]\n",
    "spam_features = vec.get_feature_names()\n",
    "\n",
    "plt.barh(spam_features, spam_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Create a pipeline with a new** `CountVectorizer()` **and a** `MultinomialNB()` **model. You will need to:** \n",
    "\n",
    "* Create the pipeline\n",
    "* Fit the model to the training set\n",
    "* Score the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = (make_pipeline(CountVectorizer(), ConfusionMatrix(MultinomialNB())))\n",
    "pipe.fit(X_train, y_train)\n",
    "print(pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. The model's** `alpha` **parameter adjusts probability for complete misses, where a feature does not appear at all.**  \n",
    "It's important to have *some* alpha to avoid multiplying by 0. The default alpha is 1.\n",
    "\n",
    "**Use a validation curve to choose a good value for** `alpha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe25 = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "curve = ValidationCurve(pipe25, 'multinomialnb__alpha', \n",
    "                        [0.001, 0.005, 0.1, 0.2])\n",
    "curve.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Multi-Class and Twitter Bot \n",
    "\n",
    "### Warm Ups\n",
    "\n",
    "*Type the given code into the cell below*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%pip install tweepy\n",
    "import tweepy\n",
    "from random import choices\n",
    "\n",
    "tweets = pd.read_csv(\"tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "---\n",
    "\n",
    "**1. Create a pipeline with a new CountVectorizer() saved as** `vec` **and a MultinomialNB() saved as** `model` **and train it on** `tweets` \n",
    "\n",
    "**What is the accuracy of the model?**\n",
    "\n",
    "Hint: For easy access, create `vec` and `model` before placing them in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tweets['content']\n",
    "y = tweets['author']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "vec = CountVectorizer()\n",
    "model = MultinomialNB()\n",
    "pipe = make_pipeline(vec, model)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Plot a** `ConfusionMatrix` **of your model. Which celebrity is hardest to classify?**         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "model = MultinomialNB()\n",
    "pipe=make_pipeline(vec,ConfusionMatrix(model))\n",
    "pipe.fit(X_train, y_train)\n",
    "print(pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. CountVectorizer allows for many parameters to improve the quality of data it produces for your model. Use the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to fine-tune your model with different combinations**\n",
    "\n",
    "Note: We will learn how to automate this process with Grid Search in the next class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(stop_words='english', ngram_range=(1,3), min_df=2)\n",
    "model = MultinomialNB()\n",
    "pipe = make_pipeline(vec, model)\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. The code below will select words at random, according to how frequently they're used by the chosen celebrity**\n",
    "\n",
    "**Change the** `celeb_index` **and** `num_words` **to get different randomly generated tweets**\n",
    "\n",
    "Hint: Use `model.classes_` to choose a specific celebrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeb_index = 0\n",
    "num_words = 4\n",
    "\n",
    "words = choices(vec.get_feature_names(),\n",
    "                weights=model.feature_count_[celeb_index],\n",
    "                k=num_words)\n",
    "\n",
    "\" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Run the code below to set up Tweepy**\n",
    "\n",
    "**Pass a string to** `api.update_status()` **to send a tweet:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler('8hQMoNize5oshYZjaqai1DyVR', \n",
    "                           'UhaEatmV6a0PdeuM1RWnsMNOJFHEmyNcFpsYb46czvnPJtzF1E')\n",
    "auth.set_access_token('3629445439-YGOjZU7i4a455eDEtlUMmnx4koOYi0nax9ixGB7', \n",
    "                      'eEwqBLLWKIHZAU9WwOFgAfRhclTNpUYTRG6j4k3EaJCkO')\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Add your code down here\n",
    "api.update_status(\"Testing testing 123 -- student tweets incoming!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
